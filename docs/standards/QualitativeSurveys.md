# Qualitative Surveys (Interview Studies) 
<standard name="Qualitative Surveys (Interview Studies)">



_<desc>Research comprising semi-structured or open-ended interviews</desc>_


## Application 

This standard applies to empirical inquiries that meet all of the
following criteria:

-   Researcher(s) have synchronous conversations with one participant at
    a time
-   Researchers ask, and participants answer, open-ended questions
-   Participants' answers are recorded in some way
-   Researchers apply some kind of qualitative data analysis to
    participants' answers

If researchers iterated between data collection and analysis, consider
the **Grounded Theory Standard**. If respondents are all from the same
organization, consider the **Case Study Standard**. If researchers
collect written text or conversations (e.g. StackExchange threads),
consider the **Discourse Analysis Standard (not yet available)**.

## Specific Attributes

### Essential Attributes	
<checklist name="Essential">

<method>    
    
- [ ]	explains how interviewees were selected (see the [Sampling Supplement](https://github.com/acmsigsoft/EmpiricalStandards/blob/master/docs/supplements/Sampling.md))
- [ ]	describes interviewees (e.g. demographics, work roles)
- [ ]   describes interviewer(s) (e.g. experience, perspective)     
    
<results>    
    
- [ ]	presents clear chain of evidence from interviewee quotations to findings (e.g. proposed concepts)
- [ ]	clearly answers the research question(s)
- [ ]	provides evidence of saturation; explains how saturation was achieved<sup><a class="footnote footnote_ref">1</a></sup>
    
<discussion>
    
 - [ ]   researchers reflect on their own possible biases
 - [ ]   identifies key issues under consideration
    
</checklist>
     
### Desirable Attributes	
<checklist name="Desirable">

- [ ]	provides supplemental materials including interview guide(s), coding schemes, coding examples, decision rules, or extended chain-of-evidence table(s)
- [ ]   describes questions asked in data collection: content of central questions, form of questions (e.g. open vs. closed)
- [ ]	includes highly diverse participants
- [ ]   describes the relationships and interactions between researchers and participants relevant to the research process
- [ ]	uses direct quotations extensively to support key points
- [ ]   identifies data recording methods (audio/visual), field notes or transcription processes used
- [ ]	EITHER: evaluates an a priori theory (or model, framework, taxonomy, etc.) using deductive coding with an a priori coding scheme based on the prior theory    
     OR: synthesizes results into a new, mature, fully-developed and clearly articulated theory (or model, etc.) using some form of inductive coding (coding scheme generated from data)
- [ ]   validates results using member checking, dialogical interviewing, feedback from non-participant practitioners or research audits of coding by advisors or other researchers<sup><a class="footnote footnote_ref">2</a></sup>)
- [ ]	discusses transferability; findings plausibly transferable to different contexts
- [ ]	compares results with (or integrates them into) prior theory or related research
- [ ]   reflects on any alternative explanations of the findings
- [ ]   provides relevant contextual information for findings
- [ ]	reflects on how researchers’ biases may have affected their analysis
- [ ]   describes any incentives or compensation, and provides assurance of relevant ethical processes of data collection and consent process as relevant
</checklist>
     
### Extraordinary Attributes	
<checklist name="Extraordinary">

- [ ]	employs multiple methods of data analysis (e.g. open coding vs. process coding; manual coding vs. automated sentiment analysis) with method-triangulation
- [ ]	employs longitudinal design (i.e. each interviewee participates multiple times) and analysis
- [ ]	employs probabilistic sampling strategy; statistical analysis of response bias
- [ ]	uses multiple coders and analyzes inter-coder reliability (see [IRR/IRA Supplement](https://github.com/acmsigsoft/EmpiricalStandards/blob/master/docs/supplements/InterRaterReliabilityAndAgreement.md))
</checklist>

## General Quality Criteria

An interview study should address appropriate qualitative quality
criteria such as: **credibility,** **resonance**, **usefulness**, and
**transferability** (see [Glossary](../glossary)). Quantitative quality criteria
such as internal validity, construct validity, generalizability and
reliability typically do not apply.

## Examples of Acceptable Deviations

-   In a study of deaf software developers, the interviews are conducted
    via text messages.
-   In a study of sexual harassment at named organizations, detailed
    description of interviewees and direct quotations are omitted to
    protect participants.
-   In a study of barriers faced by gay developers, participants are all
    gay (but should be diverse on other dimensions).

## Antipatterns 

-   Interviewing a small number of similar people, creating the illusion
    of convergence and saturation
-   Mis-presenting a qualitative survey as grounded theory or a case
    study.

## Invalid Criticisms 

-   Lack of quantitative data; causal analysis; objectivity, internal
    validity, reliability, or generalizability.
-   Lack of replicability or reproducibility; not releasing transcripts.
-   Lack of probability sampling, statistical generalizability or
    representativeness unless representative sampling was an explicit
    goal of the study.
-   Failure to apply grounded theory or case study practices. A
    qualitative survey is not grounded theory or a case study.

## Notes 

-   A qualitative survey generally has more interviews than a case study
    that triangulates across different kinds of data.

## Suggested Readings 

Virginia Braun and Victoria Clarke. "Using thematic analysis in psychology." _Qualitative Research in Psychology_ 3, 2 (2006), 77-101.
    
Khaldoun M. Aldiabat and Carole-Lynne Le Navenec. "Data saturation: the mysterious step in Grounded Theory Methodology." _The Qualitative Report_ 23, 1 (2018), 245-261.
    
Michael Quinn Patton. 2002. *Qualitative Research and Evaluation Methods*. 3rd ed. Sage.

Herbert J. Rubin and Irene S. Rubin. 2011. *Qualitative Interviewing: The Art of Hearing Data*. Sage.

Russel Walsh. “The methods of reflexivity.” *The Humanistic Psychologist* 31, 4 (2003), 51–66. doi:10.1080/08873267.2003.9986934.

Johnny Saldaña. 2015. *The coding manual for qualitative researchers*. Sage.

## Exemplars 

Marian Petre. 2013. UML in practice. In *Proceedings of the 35th International Conference on Software Engineering,* San Francisco, USA, 722–731.

Paul Ralph and Paul Kelly. 2014. The dimensions of software engineering success. In *Proceedings of the 36th International Conference on Software Engineering (ICSE 2014)*. Association for Computing Machinery, New York, NY, USA, 24–35. DOI: 10.1145/2568225.2568261

Paul Ralph and Ewan Tempero. 2016. Characteristics of decision-making during coding. In *Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering (EASE '16).* Association for Computing Machinery, New York, NY, USA, Article 34, 1–10. DOI:10.1145/2915970.2915990

---
<footnote><sup><a class="footnote footnote_text">1</a></sup>cf. Khaldoun M. Aldiabat and Carole-Lynne Le Navenec. "Data Saturation: The Mysterious Step in Grounded Theory Methodology." _The Qualitative Report_, vol. 23, no. 1, 2018, pp. 245-261.</footnote><br> 
<footnote><sup><a class="footnote footnote_text">2</a></sup>L. Harvey. 2015. Beyond member-checking: A dialogic approach to the research interview, International Journal of Research & Method in Education, 38, 1, 23–38.</footnote><br>
</standard>
