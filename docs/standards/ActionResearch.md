# Action Research 
<standard name="Action Research">



*<desc>Empirical research that investigates how an intervention, like the
introduction of a method or tool, affects a real-life context</desc>*



## Application 

This standard applies to empirical research that meets the following
conditions.

-   investigates a primarily social phenomenon within its real-life,
    organizational context
-   intervenes in the real-life context (otherwise see the **Case Study
    Standard**)
-   the change and its observation are an integral part of addressing
    the research question and contribute to research

If the intervention primarily alters social phenomena (e.g. the
organization's processes, culture, way of working or group dynamics),
use this standard. If the intervention is a new technology or technique
(e.g. a testing tool, a coding standard, a modeling grammar), especially
if it lacks a social dimension, consider the **Engineering Research
Standard**. If the research involves creating a technology and an
organizational intervention with a social dimension, consider both
standards.

## Specific Attributes
### Essential Attributes
<checklist name="Essential">
    
<intro>


<method>
    
- [ ]   justifies the selection of the site(s) that was(were) studied
- [ ]   describes the site(s) in rich detail     
- [ ]   describes the relationship between the researcher and the host organization<sup><a class="footnote footnote_ref">1</a></sup>
- [ ]   describes the intervention(s) in detail
- [ ]   describes how interventions were determined (e.g. by management, researchers, or a participative/co-determination process)
- [ ]   explains how the interventions are evaluated<sup><a class="footnote footnote_ref">2</a></sup>
- [ ]   describes the longitudinal dimension of the research design (including the length of the study)
- [ ]   describes the interactions between researcher(s) and host organization(s)<sup><a class="footnote footnote_ref">3</a></sup>
- [ ]   explains research cycles or phases, if any, and their relationship to the intervention(s)<sup><a class="footnote footnote_ref">4</a></sup>

<results>
    
- [ ]   presents a clear chain of evidence from observations to findings
- [ ]   reports participant or stakeholder reactions to interventions  
    
<discussion>

- [ ]   reports lessons learned by the organization
- [ ]   researchers reflect on their own possible biases

<other>

</checklist>
    
### Desirable Attributes
<checklist name="Desirable">
    
- [ ]	provides supplemental materials such as interview guide(s), coding schemes, coding examples, decision rules, or extended chain-of-evidence tables
- [ ]	uses direct quotations extensively
- [ ]   validates results using member checking, dialogical interviewing<sup><a class="footnote footnote_ref">5</a></sup>, feedback from non-participant practitioners or research audits of coding by advisors or other researchers
- [ ]   findings plausibly transferable to other contexts
- [ ]   triangulation across quantitative and qualitative data
</checklist>
    
### Extraordinary Attributes
<checklist name="Extraordinary">

- [ ]	research team with triangulation across researchers (to mitigate researcher bias)
</checklist>
     
## General Quality Criteria 

Example criteria include reflexivity, credibility, resonance, usefulness
and transferability (see [Glossary](../glossary)). Positivist quality criteria such
as internal validity, construct validity, generalizability and
reliability typically do not apply.

## Examples of Acceptable Deviations 

-   In a study of deviations from organizational standards, detailed
    description of circumstances and direct quotations are omitted to
    protect participants.
-   The article reports a negative outcome of an intervention and e.g.
    investigates why a certain method was not applicable.

## Antipatterns 

-   Forcing interventions that are not acceptable to participants or the
    host organization.
-   Losing professional distance and impartiality; getting too involved
    with the community under study.
-   Over-selling a tool or method without regard for participants'
    problems, practices or values.
-   Avoiding systematic evaluation; downplaying problems; simply
    reporting participants views of the intervention.

## Invalid Criticisms 

-   The findings and insights are not valid because the research
    intervened in the context. Though reflexivity is crucial, the whole
    point of action research is to introduce a change and observe how
    participants react.
-   This is merely consultancy or an experience report. Systematic
    observation and reflection should not be dismissed as consultancy or
    experience reports. Inversely, consultancy or experiences should not
    be falsely presented as action research.
-   Lack of quantitative data; causal analysis; objectivity, internal
    validity, reliability, or generalizability.
-   Sample not representative; lack of generalizability; generalizing
    from one organization.
-   Lack of replicability or reproducibility; not releasing transcripts.
-   Lack of control group or experimental protocols. An action research
    study is not an experiment.

## Suggested Readings

Richard Baskerville and A. Trevor Wood-Harper. 1996. A critical
perspective on action research as a method for information systems
research.\" *Journal of information Technology* 11.3, 235–246.

Peter Checkland and Sue Holwell. 1998. Action Research: Its Nature and
Validity. *Systematic Practice and Action Research.* (Oct. 1997), 9–21.

Yvonne Dittrich. 2002. Doing Empirical Research on Software Development:
Finding a Path between Understanding, Intervention, and Method
Development. In *Social thinking---Software practice*. 243–262

Yvonne Dittrich, Kari Rönkkö, Jeanette Eriksson, Christina Hansson and
Olle Lindeberg. 2008. Cooperative method development. *Empirical
Software Engineering.* 13, 3, 231-260. DOI: 10.1007/s10664-007-9057-1

Kurt Lewin. 1947. Frontiers in Group Dynamics. *Human Relations* 1, 2
(1947), 143--153. DOI: 10.1177/001872674700100201

Lars Mathiassen. 1998. Reflective systems development. *Scandinavian
Journal of Information Systems* 10, 1 (1998), 67–118

Lars Mathiassen. 2002. Collaborative practice research. *Information,
Technology & People.* 15,4 (2002), 321–345

Lars Mathiassen, Mike Chiasson, and Matt Germonprez. 2012. Style
Composition in Action Research Publication. *MIS quarterly. JSTOR* 36, 2
(2012), 347-363

Miroslaw Staron. Action research in software engineering: Metrics'
research perspective. *International Conference on Current Trends in
Theory and Practice of Informatics.* (2019), 39-49

Maung K. Sein, Ola Henfridsson, Sandeep Purao, Matti Rossi and Rikard
Lindgren. 2011. Action design research. *MIS quarterly*. (2011), 37-56.
DOI: 10.2307/23043488

## Exemplars

Yvonne Dittrich, Kari Rönkkö, Jeanette Eriksson, Christina Hansson and
Olle Lindeberg. 2008. Cooperative method development. *Empirical
Software Engineering*. 13, 3 (Dec. 2007), 231-260. DOI:
10.1007/s10664-007-9057-1

Helle Damborg Frederiksen, Lars Mathiassen. 2005. Information-centric
assessment of software metrics practices. IEEE Transactions on
Engineering Eanagement. 52, 3 (2005), 350-362. DOI:
10.1109/TEM.2005.850737

Jakob Iversen and Lars Mathiassen. 2003. Cultivation and engineering of
a software metrics program. Information Systems Journal. 13, 1 (2006),
3--19

Jakob Iversen. 1998. Problem diagnosis software process improvement.
Larsen TJ, Levine L, DeGross JI (eds) Information systems: current
issues and future changes.

Martin Kalenda, Petr Hyna, Bruno Rossi. *Scaling agile in large
organizations: Practices, challenges, and success factors*. Journal of
Software: Evolution and Process. Wiley Online Library 30, 10 (Oct.
2018), 1954 pages.

Miroslaw Ochodek, Regina Hebig, Wilhem Meding, Gert Frost, Miroslaw
Staron. Recognizing lines of code violating company-specific coding
guidelines using machine learning. *Empirical Software Engineering*. 25,
1 (Jan. 2020), 220-65.

Kari Rönkkö, Brita Kilander, Mats Hellman, Yvonne Dittrich. 2004.
Personas is not applicable: local remedies interpreted in a wider
context. In *Proceedings of the eighth conference on Participatory
design: Artful integration: interweaving media, materials and
practices-Volume 1, Toronto, ON*, 112--120.

Thatiany Lima De Sousa, Elaine Venson, Rejane Maria da Costa Figueired,
Ricardo Ajax Kosloski, and Luiz Carlos Miyadaira Ribeiro. Using Scrum in
Outsourced Government projects: An Action Research. 2016. In *2016 49th
Hawaii International Conference on System Sciences (HICSS)*, January 5,
2016, 5447-5456.

Hataichanok Unphon, Yvonne Dittrich. 2008. Organisation matters: how the
organisation of software development influences the introduction of a
product line architecture. In *Proc. IASTED Int. Conf. on Software
Engineering*. 2008, 178-183.

---
<footnote><sup><a class="footnote footnote_text">1</a></sup>E.g. project financing, potential conflicts of interest, professional relationship leading to access.</footnote><br>
<footnote><sup><a class="footnote footnote_text">2</a></sup>Can include quantitative evaluation in addition to qualitative evaluation.</footnote><br>
<footnote><sup><a class="footnote footnote_text">3</a></sup>i.e. who intervened and with which part of the organization? </footnote><br>
<footnote><sup><a class="footnote footnote_text">4</a></sup>Action research projects are structured in interventions often described as action research cycles, which are often structured in distinct phases. It is a flexible methodology, where subsequent cycles are based on their predecessors.</footnote><br>
<footnote><sup><a class="footnote footnote_text">5</a></sup>L. Harvey. 2015. Beyond member-checking: A dialogic approach to the research interview, International Journal of Research & Method in Education, 38, 1, 23–38.</footnote><br>
</standard>
