#  Multi-Methodology and Mixed Methods Research

<standard name="Mixed Methods">

*Studies that use two or more approaches to data collection or analysis to corroborate, complement and expand research findings (multi-methodology) or combine and integrate inductive research with deductive research (mixed methods), often but not necessarily relying on qualitative and/or quantitative data.*


## Application

This standard applies to software engineering studies that use two ir more data collection or analysis methods. It enumerates criteria related to 
the mixing of methodologies, not the methodologies themselves. For the latter, refer to method-specific standards. For example, a multi-methodology study combining a case study with an experiment should comply with **The Case Study Standard** and **The Experiment Standard** as well as this standard. 

## Specific Attributes

### Essential
<checklist name="Essential">

- [ ] **justifies** using multiple methodologies and/or methods
- [ ] provides a **purpose statement** that conveys the overarching multi or 
  mixed method design intent (why)
- [ ] describes the **multi-methodology**, **multi-method** or **mixed method** design (what) 
- [ ] describes **which phases** of the research study the different methods or 
  methodologies are used in (when)
- [ ] describes how the design aligns with the research question or objective
- [ ] **integrates the findings** from all methods to address the research 
  question/objective
- [ ] acknowledges the **limitations** associated with integrating findings (e.g., samples that are drawn from different populations may introduce limitations when the findings are integrated, or biases may be introduced by the sequential or parallel nature of a mixed design).
 </checklist>

### Desirable
<checklist name="Desirable">

- [ ] **defines** the multi-methodology or mixed method design used
- [ ] describes and justifies **sample reuse** (or no reuse, or partial reuse) across 
  methods
- [ ] illustrated the research design using a **visual model** (diagram)
- [ ] indicates the use of multiple methods or mixed method design in the **title** 
- [ ] (for mixed-methods) includes, in the **literature review**, a mixture of quantitative, 
  qualitative, and mixed methods related work
- [ ] **distinguishes the additional value** from using a multi-methodology or mixed 
  method design in terms of corroboration, complementarity, and expansion 
  (breadth and depth)
- [ ] discusses **discrepancies and incongruent findings** from the use of multiple 
  methods
- [ ] describes the main **philosophical, epistemological, and/or theoretical foundations** 
  that the authors use and relate those to the planned use of multi or mixed methods in the study
- [ ] describes the **challenges** faced in the design and how those were or could be 
  mitigated
- [ ] describes how the methods and their findings relate to one or more 
      **theories or theoretical frameworks**
- [ ] describes **ethical issues** that may have been presented through the blend of 
  multi- or mixed methods
 </checklist>

### Extraordinary
<checklist name="Extraordinary">

- [ ] contributes to the methodological discourse surrounding multi-methodology or mixed-methods
 </checklist>

## General Quality Criteria

General quality criteria discussed in the guidelines for each method should be 
considered.  In the case of a multi-methodology or multi-method design, the 
reliability of the findings that are specific to the triangulation goals should 
be assessed.  In the case of a mixed method design, the research may also 
require a “legitimation step to assess the trustworthiness of both the 
qualitative and quantitative data and subsequent interpretations” 
[Johnson/On., P. 22].

## Antipatterns

- **Uninvited guest:** A research method is not clearly introduced in the paper 
  introduction/methodology and makes an unexpected entrance in the discussion or 
  limitations sections of the paper
- **Smoke and mirrors:** Overselling a study as a multi-methodology or mixed method design 
  when one approach at best offers a token or anecdotal contribution to the research motivation 
  or findings
- **Selling your soul:** Employing an additional method to appeal to a methodological purist 
  during the review process that does not contribute substantively to the research findings
- **Integration failure:** Poor integration of findings from all methods used
- **Limitation shirker**: Failure to discuss limitations from all methods used or from their 
  integration
- **Missing the mark:** Misalignment of multi- or mixed method design with the research 
  question/objective
- **Cargo cult research:** Using methods where the research team lacks expertise in those methods, 
  but hopes they work
- **Design by committee:** Failure to agree on a crisp research question/objective (may be 
   induced by different epistemological perspectives or use of heterogeneous methods)
- **Golden hammer:** relying on superficial, typically quantitative analysis of rich qualitative data 
- **Sample contamination:** a mixed method sequential design where the same participants are used in multiple,
     sequential methods without accounting for potential contamination from earlier method(s) to later ones.
- **Ignoring the writing on the wall:** In a mixed method sequential design, failing to use 
  findings from an earlier study when forming an instrument for a study in a later phase of 
  the research

## Examples of Acceptable Deviations

- Conference page limits may make it particularly challenging to share sufficient details 
  on all methods used. These details may be available in other publications or supplementary 
  materials.  
- Describing the research in terms of cycles or ongoing parallel processes instead of phases.


## Invalid Criticisms

- The method(ologie)s do not contribute equally (a non-equal design) or the minor method is limited (e.g. few participants).
- The mixed- or multi-method approach isn't necessary (when it *is* beneficial)
- The method(ologie)s have different philosophical foundations or are otherwise incompatible
- In an unequal design, the wrong method is dominant (this is a study design choice not a flaw )
- The method(ologie)s have inconsistent findings

## Notes

- Multi-methodology research is sometimes referred to as multi-method, blended research, 
  or integrative research (see P. 118 Johnson 2008).  Mixed method research is 
  sometimes seen as a special case of multi-methodology research that blends 
  deductive and inductive research, or mixes methods that rely on quantitative 
  and qualitative data. 
- The term "method" may refer to a way of collecting data or a broader process of doing research. 
  In mixed methods research, "method" means the broader process of doing research. 
- The term "methodology" may refer to an approach to doing research or the study of how research is done.
   In "multi-methodology," methodology means a process of doing research  
- For multi-method research, it is important to distinguish between using different methods 
  of collecting data (e.g., as part of an inductive case study) and using different research 
  strategies (e.g., complementing an inductive case study with a deductive experiment in a 
  later stage of the research).
- Some experts on mixed methods use specific terms to refer to core mixed method designs 
  (e.g., Convergent mixed methods, Explanatory sequential mixed methods, Exploratory sequential 
  mixed methods are defined by Creswell & Creswell (2007) and Quantitatively driven approaches/designs, 
  Qualitatively driven approaches/designs, Interactive or equal status designs are used by 
  Johnson, Onwuegbuzie, & Turner, 2007).   

## Suggested Readings

1. Bergman, M. M. (2011). The good, the bad, and the ugly in mixed methods research and design. 
Journal of Mixed Methods Research, 5(4), 271-275. doi:10.1177/1558689811433236

2. Creswell, John W., V. L. Plano Clark, Michelle L. Gutmann, and William E. Hanson. 
"An expanded typology for classifying mixed methods research into designs." 
A. Tashakkori y C. Teddlie, Handbook of mixed methods in social and behavioral research (2003): 
209-240.

3. Easterbrook, Steve, Janice Singer, Margaret-Anne Storey, and Daniela Damian. 
"Selecting empirical methods for software engineering research." In Guide to advanced 
empirical software engineering, pp. 285-311. Springer, London, 2008.

4. Johnson, R. & Onwuegbuzie, Anthony & Turner, Lisa. (2007). Toward a Definition of Mixed Methods 
Research. Journal of Mixed Methods Research, 1, 112-133. Journal of Mixed Methods Research. 1. 112 -133. 10.1177/1558689806298224.

5. Ladner, S., Mixed Methods: A short guide to applied mixed methods design, 2019. 
https://www.mixedmethodsguide.com/ 

6. O'Cathain, A. (2010). Assessing the quality of mixed methods research: Towards a comprehensive 
framework. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and 
behavioral research (2nd edition) (pp. 531-555). Thousand Oaks: Sage.

7. Pace, R., Pluye, P., Bartlett, G., Macaulay, A., Salsberg, J., Jagosh, J., & Seller, R. (2010). 
Reliability of a tool for concomitantly appraising the methodological quality of qualitative, 
quantitative and mixed methods research: a pilot study. 38th Annual Meeting of the North 
American Primary Care Research Group (NAPCRG), Seattle, USA.

8. Pluye, P., Gagnon, M.P., Griffiths, F. & Johnson-Lafleur, J. (2009). A scoring system for 
appraising mixed methods research, and concomitantly appraising qualitative, quantitative and 
mixed methods primary studies in Mixed Studies Reviews. International Journal of Nursing Studies,
46(4), 529-46.

9. Storey, MA., Ernst, N.A., Williams, C. et al. The who, what, how of software engineering 
research: a socio-technical framework. Empir Software Eng 25, 4097–4129 (2020). 
https://doi.org/10.1007/s10664-020-09858-z

10. Tashakkori, Abbas, and John W. Creswell. "The new era of mixed methods." (2007): 3-7.

11. Teddlie, Charles, and Abbas Tashakkori. "A general typology of research designs featuring 
mixed methods." Research in the Schools 13, no. 1 (2006): 12-28.

12. https://en.wikipedia.org/wiki/Multimethodology 

## Exemplars 

| **Paper reference** | **Methods used** |
|------------------------------------- | ---------------- |
|M. Almaliki, C. Ncube and R. Ali, "The design of adaptive acquisition of users feedback: An empirical study," 2014 IEEE Eighth International Conference on Research Challenges in Information Science (RCIS), Marrakech, 2014, pp. 1-12, doi: 10.1109/RCIS.2014.6861076.|Questionnaire+interviews|
|Sebastian Baltes and Stephan Diehl. 2019. Usage and attribution of Stack Overflow code snippets in GitHub projects. Empirical Softw. Engg. 24, 3 (June 2019), 1259–1295. DOI:https://doi.org/10.1007/s10664-018-9650-5 [[pdf]](https://empirical-software.engineering/assets/pdf/emse18-snippets.pdf)|Mining clones of SO code + qualitative analysis + follow-up online survey|
|Sebastian Baltes and Stephan Diehl. 2018. Towards a theory of software development expertise. In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2018). Association for Computing Machinery, New York, NY, USA, 187–200. DOI:https://doi.org/10.1145/3236024.3236061 [[pdf]](https://empirical-software.engineering/assets/pdf/fse18-expertise.pdf)|Combining deductive and inductive theory building steps|
|T. Barik, Y. Song, B. Johnson and E. Murphy-Hill, "From Quick Fixes to Slow Fixes: Reimagining Static Analysis Resolutions to Enable Design Space Exploration," 2016 IEEE International Conference on Software Maintenance and Evolution (ICSME), Raleigh, NC, 2016, pp. 211-221, doi: 10.1109/ICSME.2016.63. [[pdf]](http://static.barik.net/barik/fixbugs/PID4404509.pdf) [[blogpost]](http://static.barik.net/barik/fixbugs/)|This had both a usability tool study but also a heuristic evaluation (!) using expert evaluators.|
|M. Borg, K. Wnuk, B. Regnell and P. Runeson, "Supporting Change Impact Analysis Using a Recommendation System: An Industrial Case Study in a Safety-Critical Context," in IEEE Transactions on Software Engineering, vol. 43, no. 7, pp. 675-700, 1 July 2017, doi: 10.1109/TSE.2016.2620458.|Spent a month at a site in India. Replayed some development history, instrumented and measured, did interviews.|
| Chattopadhyay, Souti, Nicholas Nelson, Audrey Au, Natalia Morales, Christopher Sanchez, Rahul Pandita, and Anita Sarma. "A tale from the trenches: cognitive biases and software development." In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, pp. 654-665. 2020. | Observations + interviews |
| F. Ebert, F. Castor, N. Novielli and A. Serebrenik, "Confusion in Code Reviews: Reasons, Impacts, and Coping Strategies," 2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER), Hangzhou, China, 2019, pp. 49-60, doi: 10.1109/SANER.2019.8668024. [[pdf]](https://www.win.tue.nl/~aserebre/SANER2019Felipe.pdf)| Survey + repository mining on confusion | 
|Egelman, Carolyn D., Emerson Murphy-Hill, Elizabeth Kammer, Margaret Morrow Hodges, Collin Green, Ciera Jaspan, and James Lin. "Predicting developers' negative feelings about code review." In 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE), pp. 174-185. IEEE, 2020. [[pdf]](https://research.google/pubs/pub49023/) |Surveys cross-referenced with code review data.|
|D. Ford, M. Behroozi, A. Serebrenik and C. Parnin, "Beyond the Code Itself: How Programmers Really Look at Pull Requests," 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Society (ICSE-SEIS), Montreal, QC, Canada, 2019, pp. 51-60, doi: 10.1109/ICSE-SEIS.2019.00014. [[pdf]](http://denaeford.me/papers/beyond-ICSE-SEIS-2019.pdf)|Eye tracking software and interviews with developers to figure out how they evaluate contributions to OSS|
|Fabian Gilson, Miguel Morales-Trujillo, and Moffat Mathews. 2020. How junior developers deal with their technical debt? In Proceedings of the 3rd International Conference on Technical Debt (TechDebt '20). Association for Computing Machinery, New York, NY, USA, 51–61. DOI:https://doi.org/10.1145/3387906.3388624|They looked at how students (proxy for junior devs) deal with their tech debt using a survey, some analysis of their smells (sonarqube + commits) and a focus group.|
|Austin Z. Henley, KΙvanç Muçlu, Maria Christakis, Scott D. Fleming, and Christian Bird. 2018. CFar: A Tool to Increase Communication, Productivity, and Review Quality in Collaborative Code Reviews. Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, USA, Paper 157, 1–13. DOI:https://doi.org/10.1145/3173574.3173731[[pdf]](https://web.eecs.utk.edu/~azh/pubs/Henley2018CHI_CFar.pdf)|Looked at the effects of an automated code reviewer on team collaboration at Microsoft using a lab study, a field study, and a survey. |
|Yogeshwar Shastri, Rashina Hoda, Robert Amor,"The role of the project manager in agile software development projects", Journal of Systems and Software, Volume 173, 2021, 110871, ISSN 0164-1212, https://doi.org/10.1016/j.jss.2020.110871. [[pdf]](https://www.researchgate.net/publication/346556944_The_Role_of_the_Project_Manager_in_Agile_Software_Development_Projects)|A gentle approach to mixed methods with GT, primarily qual with supplementary quant.|
|Thomas D. LaToza, Gina Venolia, and Robert DeLine. 2006. Maintaining mental models: a study of developer work habits. In Proceedings of the 28th international conference on Software engineering (ICSE '06). Association for Computing Machinery, New York, NY, USA, 492–501. DOI:https://doi.org/10.1145/1134285.1134355 [[pdf]](https://cs.gmu.edu/~tlatoza/papers/icse2006.pdf)|survey + interviews + survey|
|Thomas D. LaToza and Brad A. Myers. 2010. Developers ask reachability questions. Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1. Association for Computing Machinery, New York, NY, USA, 185–194. DOI:https://doi.org/10.1145/1806799.1806829 [[pdf]](https://t.co/BEMOUqsYcn?amp=1)|lab obs + survey + field obs|
|C. Omar, Y. S. Yoon, T. D. LaToza and B. A. Myers, "Active code completion," 2012 34th International Conference on Software Engineering (ICSE), Zurich, 2012, pp. 859-869, doi: 10.1109/ICSE.2012.6227133. [[pdf]](https://t.co/7Y8g2NYTRv?amp=1)|survey + lab study|
|Nicolas Mangano, Thomas D. LaToza, Marian Petre, and André van der Hoek. 2014. Supporting informal design with interactive whiteboards. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '14). Association for Computing Machinery, New York, NY, USA, 331–340. DOI:https://doi.org/10.1145/2556288.2557411 [[pdf]](https://t.co/lchwHSh0mD?amp=1)|lit review + logs + interviews|
|Thomas D. LaToza, W. Ben Towne, Christian M. Adriano, and André van der Hoek. 2014. Microtask programming: building software with a crowd. In Proceedings of the 27th annual ACM symposium on User interface software and technology (UIST '14). Association for Computing Machinery, New York, NY, USA, 43–54. DOI:https://doi.org/10.1145/2642918.2647349 [[pdf]](https://cs.gmu.edu/~tlatoza/papers/uist2014.pdf)|logs + survey 2x|
|L. Martie, T. D. LaToza and A. v. d. Hoek, "CodeExchange: Supporting Reformulation of Internet-Scale Code Queries in Context (T)," 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE), Lincoln, NE, 2015, pp. 24-35, doi: 10.1109/ASE.2015.51. [[pdf]](https://t.co/m1Jm6Rl772?amp=1)|field deployment + lab study|
|Thomas D. LaToza, W. Ben Towne, Christian M. Adriano, and André van der Hoek. 2014. Microtask programming: building software with a crowd. In Proceedings of the 27th annual ACM symposium on User interface software and technology (UIST '14). Association for Computing Machinery, New York, NY, USA, 43–54. DOI:https://doi.org/10.1145/2642918.2647349 [[pdf]](https://t.co/7zqPTkbydO?amp=1)|log data + surveys|
|A. Alaboudi and T. D. LaToza, "An Exploratory Study of Live-Streamed Programming," 2019 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC), Memphis, TN, USA, 2019, pp. 5-13, doi: 10.1109/VLHCC.2019.8818832. [[pdf]](https://t.co/XgYXiDZwWa?amp=1)|web videos + interviews|
|K. Chugh, A. Y. Solis and T. D. LaToza, "Editable AI: Mixed Human-AI Authoring of Code Patterns," 2019 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC), Memphis, TN, USA, 2019, pp. 35-43, doi: 10.1109/VLHCC.2019.8818871. [[pdf]](https://t.co/kgFoBLCQRg?amp=1)|lab study + interviews|
|E. Aghayi, A. Massey and T. LaToza,  "Find Unique Usages: Helping Developers Understand Common Usages," in 2020 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC), Dunedin, New Zealand, 2020 pp. 1-8.doi: 10.1109/VL/HCC50065.2020.9127285 https://doi.ieeecomputersociety.org/10.1109/VL/HCC50065.2020.9127285[[pdf]](https://t.co/RWL3MjcdGZ?amp=1)|formative obs + lab study|
|Clara Mancini, Keerthi Thomas, Yvonne Rogers, Blaine A. Price, Lukazs Jedrzejczyk, Arosha K. Bandara, Adam N. Joinson, and Bashar Nuseibeh. 2009. From spaces to places: emerging contexts in mobile privacy. In Proceedings of the 11th international conference on Ubiquitous computing (UbiComp '09). Association for Computing Machinery, New York, NY, USA, 1–10. DOI:https://doi.org/10.1145/1620545.1620547 [[pdf]](https://t.co/ErsTfHbwuB?amp=1)| Extended experience sampling with ‘memory phrases’ to allow for deferred contextual interviews, to elicit mobile privacy reqs|
|Clara Mancini, Yvonne Rogers, Keerthi Thomas, Adam N. Joinson, Blaine A. Price, Arosha K. Bandara, Lukasz Jedrzejczyk, and Bashar Nuseibeh. 2011. In the best families: tracking and relationships. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '11). Association for Computing Machinery, New York, NY, USA, 2419–2428. DOI:https://doi.org/10.1145/1978942.1979296 [[pdf]](https://ulir.ul.ie/bitstream/handle/10344/1773/2011_Mancini.pdf?sequence=2)|They mixed ‘extended experience sampling’ from above ubicomp’09 work with ‘breaching experiments’ from psychology, to explore progressively uncomfortable invasions of privacy of user, to better understand privacy requirements.|
|Clara Mancini, Yvonne Rogers, Arosha K. Bandara, Tony Coe, Lukasz Jedrzejczyk, Adam N. Joinson, Blaine A. Price, Keerthi Thomas, and Bashar Nuseibeh. 2010. Contravision: exploring users' reactions to futuristic technology. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, USA, 153–162. DOI:https://doi.org/10.1145/1753326.1753350 [[pdf]](http://oro.open.ac.uk/19587/5/pap1586-mancini.pdf)|For privacy reqs elicitation, inspired by cinema (film: Sliding Doors), they created the ‘Contravision’ method with utopian & dystopian narratives (films) of futuristic tech, to elicit reqs from focus groups that watched their film variants|
| H. S. Qiu, A. Nolte, A. Brown, A. Serebrenik and B. Vasilescu, "Going Farther Together: The Impact of Social Capital on Sustained Participation in Open Source," 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), Montreal, QC, Canada, 2019, pp. 688-699, doi: 10.1109/ICSE.2019.00078. [[pdf]](https://www.win.tue.nl/~aserebre/ICSE2019.pdf) | Surveys + repository mining on gender diversity |
| Qiu, Huilian Sophie, Yucen Lily Li, Susmita Padala, Anita Sarma, and Bogdan Vasilescu. "The signals that potential contributors look for when choosing open-source projects." Proceedings of the ACM on Human-Computer Interaction 3, no. CSCW (2019): 1-29. | Interviews + mining |
|Nischal Shrestha, Colton Botta, Titus Barik, and Chris Parnin. 2020. Here we go again: why is it difficult for developers to learn another programming language? In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (ICSE '20). Association for Computing Machinery, New York, NY, USA, 691–701. DOI:https://doi.org/10.1145/3377811.3380352 [[pdf]](http://nischalshrestha.me/docs/cross_language_interference.pdf)|Combines Stack Overflow data with qualitative interviews. It's not so much triangulation but more about seeing the same problem from different lenses. |
| Siegmund, Janet, Norman Peitek, Sven Apel, and Norbert Siegmund. "Mastering Variation in Human Studies: The Role of Aggregation." ACM Transactions on Software Engineering and Methodology (TOSEM) 30, no. 1 (2020): 1-40. | A literature survey, an in-depth statistical re-analysis, and train several classifiers on data of different human studies to demonstrate how aggregation affects results. |
|Siegmund, Norbert, Nicolai Ruckel, and Janet Siegmund. "Dimensions of software configuration: on the configuration context in modern software development." In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 338-349. 2020. (https://sws.informatik.uni-leipzig.de/wp-content/uploads/2020/05/Configuration.pdf) | Interviews, two related papers, and an application on SE subfields for building a context of research on software configuration. |
|Stors, N. and Sebastian Baltes. “Constructing Urban Tourism Space Digitally.” Proceedings of the ACM on Human-Computer Interaction 2 (2018): 1 - 29. [[pdf]](https://empirical-software.engineering/assets/pdf/cscw18-airbnb.pdf)|Interdisciplinary study combining data mining and qualitative analysis|
|Leen Lambers, Daniel Strüber, Gabriele Taentzer, Kristopher Born, and Jevgenij Huebert. 2018. Multi-granular conflict and dependency analysis in software engineering based on graph transformation. In Proceedings of the 40th International Conference on Software Engineering (ICSE '18). Association for Computing Machinery, New York, NY, USA, 716–727. DOI:https://doi.org/10.1145/3180155.3180258|They combine 4 research methods to improve the state-of-the-art of a certain software analysis: (1.) a literature survey to identify issues with available techniques (performance- and usability-related); (2.) formal methods to define new concepts and prove that we can compute them in a sound way; (3.) a tool implementation and evaluation to show performance benefits; and (4.) a user experiment to show usability benefits arising from our new concepts.|
|Bogdan Vasilescu, Daryl Posnett, Baishakhi Ray, Mark G.J. van den Brand, Alexander Serebrenik, Premkumar Devanbu, and Vladimir Filkov. 2015. Gender and Tenure Diversity in GitHub Teams. Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, USA, 3789–3798. DOI:https://doi.org/10.1145/2702123.2702549 [[pdf]](https://www.win.tue.nl/~aserebre/CHI15.pdf) | Surveys + repository mining on gender diversity |
|Umme Ayda Mannan, Iftekhar Ahmed, Carlos Jensen, and Anita Sarma. 2020. On the relationship between design discussions and design quality: a case study of Apache projects. Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. Association for Computing Machinery, New York, NY, USA, 543–555. DOI:https://doi.org/10.1145/3368089.3409707|Data mining + survey|
|Vidoni, Melina, "Evaluating Unit Testing Practices in R Packages",To appear ICSE 2021. [[pdf]](https://melvidoni.rbind.io/publication/2021-rttd-icse/) | Combined an MSR (mining software repositories) with a developers' survey. |
|A. AlSubaihin, F. Sarro, S. Black, L. Capra and M. Harman, "App Store Effects on Software Engineering Practices," in IEEE Transactions on Software Engineering (Early Access). DOI:https://doi.org/10.1109/TSE.2019.2891715. [[pdf]](https://discovery.ucl.ac.uk/id/eprint/10065145/1/TSEAlSubaihin.pdf)|Interviews + Questionnairs, Deductive, mixed method drawing from survey and case study empirical research methodologies, exploratory + descriptive |

</standard>
