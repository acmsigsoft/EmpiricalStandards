# Open Science
The practice of maximizing the accessibility and transparency of science

## Application
The open science supplement applies to all research.

## Principle

Artifacts related to a study and the paper itself should, in principle, be made available on the Internet:

- without any barrier (e.g. paywalls, registration forms, request mechanisms),
- under an appropriate [open license](https://pantonprinciples.org/) that specifies purposes for re-use and re-purposing,
- properly [archived and preserved](https://en.wikipedia.org/wiki/Research_data_archiving),

provided that there are no ethical, legal, technical, economical, or practical barriers preventing their disclosure.

## Specific Attributes

### Desirable Attributes
- [ ]  includes a section named _data availability_ (typically after conclusion)
- [ ] EITHER: links to supplementary materials   
  OR explains why materials cannot be released (reasons for limited disclosure of data should be trusted)
- [ ] includes supplementary materials such as: raw, deidentified or transformed data, extended proofs, analysis scripts, software, virtual machines and containers, or qualitative codebooks.
- [ ] archives supplementary materials on preserved digital repositories such as [zenodo.org](https://zenodo.org/), [figshare.com](http://figshare.com/), [softwareheritage.org](https://www.softwareheritage.org/), [osf.io](https://osf.io/), or institutional repositories
- [ ] releases supplementary material under a clearly-identified open license such as [CC0](https://creativecommons.org/share-your-work/public-domain/cc0/) or [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/)

## General Criteria

Rather than evaluating reproducibility or replicability in principle, reviewers should focus on the extent to which artifacts that can be released, are released.

## Invalid Criticisms

Researchers should not complain that a study involves artifacts which— for good reasons—cannot be released.

## Examples of Acceptable Deviations

- dataset is not released because it cannot be safely deidentified (e.g. interview transcripts; videos of participants)
- source code is not released because it is closed-source and belongs to industry partner

## Notes

- authors are encouraged to self-archive their pre- and post-prints in open and preserved repositories
- open science is challenging for qualitative studies; reviewers should welcome qualitative studies which open their artifacts even in a limited way
- personal or institutional websites, version control systems (e.g. GitHub), consumer cloud storage (e.g. Dropbox), and commercial paper repositories (e.g. ResearchGate; Academia.edu) do not offer properly archived and preserved data.

## Suggested Readings

Noemi Betancort Cabrera, Elke C Bongartz, Nora Dörrenbächer, Jan Goebel, Harald Kaluza, & Pascal Siegers. 2020. White Paper on implementing the FAIR principles for data in the Social, Behavioural, and Economic Sciences (No. 274). RatSWD Working Paper. [https://www.econstor.eu/handle/10419/229719](https://www.econstor.eu/handle/10419/229719)

Carlos Diego Nascimento Damasceno. 2022. Guidelines for Quality Management of Research Artifacts in Model-Driven Engineering. _MOdeling LAnguages (blog)_. Retrieved July 17, 2022 from [https://modeling-languages.com/guidelines-for-quality-management-of-research-artifacts-in-model-driven-engineering/#](https://modeling-languages.com/guidelines-for-quality-management-of-research-artifacts-in-model-driven-engineering/#)

Daniel Graziotin. 2020. SIGSOFT open science policies. Retrieved July 12, 2020 from [https://github.com/acmsigsoft/open-science-policies/blob/master/sigsoft-open-science-policies.md](https://github.com/acmsigsoft/open-science-policies/blob/master/sigsoft-open-science-policies.md)

Daniel Graziotin. 2018. How to disclose data for double-blind review and make it archived open data upon acceptance
Retrieved Feb 24, 2024 from [https://github.com/dgraziotin/disclose-data-dbr-first-then-opendata](https://github.com/dgraziotin/disclose-data-dbr-first-then-opendata)

Daniel Méndez, Daniel Graziotin, Stefan Wagner, and Heidi Seibold. 2019. Open science in software engineering. _arXiv_. [https://arxiv.org/abs/1904.06499](https://arxiv.org/abs/1904.06499)

GitHub. 2016. Making Your Code Citable. Retrieved July 12, 2020 from [https://guides.github.com/activities/citable-code/](https://guides.github.com/activities/citable-code/). (How to automatically archive a GitHub repository to Zenodo)

Figshare. How to connect Figshare with your GitHub account. Retrieved July 12, 2020 from [https://knowledge.figshare.com/articles/item/how-to-connect-figshare-with-your-github-account](https://knowledge.figshare.com/articles/item/how-to-connect-figshare-with-your-github-account) (How to automatically archive a GitHub repository to Figshare)

### Best Practices

1. Systems Research Artifacts. 2024. Artifact Packaging Guide. Retrieved May 13, 2025 from [https://sysartifacts.github.io/packaging-guide.html](https://sysartifacts.github.io/packaging-guide.html)

2. Artifact Evaluation Committees (AEC). HOWTO for AEC Submitters. Retrieved May 13, 2025 from [https://docs.google.com/document/d/1pqzPtLVIvwLwJsZwCb2r7yzWMaifudHe1Xvn42T4CcA/edit?tab=t.0](https://docs.google.com/document/d/1pqzPtLVIvwLwJsZwCb2r7yzWMaifudHe1Xvn42T4CcA/edit?tab=t.0)

3. Tianyin Xu. 2021. How Are Award‑winning Systems Research Artifacts Prepared – Part 1. Retrieved May 13, 2025 from [https://www.sigops.org/2021/how-are-award-winning-systems-research-artifacts-prepared-part-1/](https://www.sigops.org/2021/how-are-award-winning-systems-research-artifacts-prepared-part-1/)

4. Tianyin Xu. 2021. How Are Award‑winning Systems Research Artifacts Prepared – Part 2. Retrieved May 13, 2025 from [https://www.sigops.org/2021/how-are-award-winning-systems-research-artifacts-prepared-part-2/](https://www.sigops.org/2021/how-are-award-winning-systems-research-artifacts-prepared-part-2/)

5. Rohan Padhye. 2019. Artifact Evaluation: Tips for Authors. Retrieved May 13, 2025 from [https://blog.padhye.org/Artifact-Evaluation-Tips-for-Authors/](https://blog.padhye.org/Artifact-Evaluation-Tips-for-Authors/)

## Exemplar Artifacts

Below are example artifacts corresponding to some of the resarch methods described in this supplement.

#### Method Type: Quantitative

#####  Benchmarking

- Chen, M., Tan, T., Pan, M., & Li, Y. PacDroid: A Pointer-Analysis-Centric Framework for Security Vulnerabilities in Android Apps. In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE). (Artifact). Zenodo. [https://doi.org/10.5281/zenodo.14863334](https://doi.org/10.5281/zenodo.14863334)
- D. Kokkonis, M. Marcozzi, E. Decoux and S. Zacchiroli, "ROSA: Finding Backdoors with Fuzzing," in 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE). Artifact). Zenodo. [https://zenodo.org/records/14724251](https://zenodo.org/records/14724251)

#####  Experiment

- T. R. Schorlemmer, K. G. Kalu, L. Chigges, et al. “Signing in four public software package registries: Quantity, quality, and influencing factors.” In 2024 IEEE Symposium on Security and Privacy (SP), Los Alamitos, CA, USA. (Artifact) Github [https://github.com/PurdueDualityLab/signature-adoption](https://github.com/PurdueDualityLab/signature-adoption)
- Ma, Yimeng, Yu Huang, and Kevin Leach. "Breaking the Flow: A Study of Interruptions During Software Engineering Activities."  In Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering(ICSE). (Artifact) Figshare [https://doi.org/10.6084/m9.figshare.24944568.v2](https://doi.org/10.6084/m9.figshare.24944568.v2)

#####  Repository Mining

- Inchen Wang, Ruida Hu, Cuiyun Gao, Xin-Cheng Wen, Yujia Chen, and Qing Liao. ReposVul: A Repository-Level High-Quality Vulnerability Dataset. In Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering(ICSE). (Artifact) Github [https://github.com/Eshe0922/ReposVul](https://github.com/Eshe0922/ReposVul)
- Nimmi Rashinika Weeraddana, Mahmoud Alfadel, and Shane McIntosh. Dependency-Induced Waste in Continuous Integration: An Empirical Study of Unused Dependencies in the npm Ecosystem.  In 2024 ACM Foundations of Software Engineering (FSE). (Artifact) Zenodo [https://zenodo.org/records/11192753](https://zenodo.org/records/11192753)

#### Method Type: General

#####  Mixed Methods

- Miao Miao, Austin Mordahl, Dakota Soles, Alice Beideck, Shiyi Wei. "An Extensive Empirical Study of Nondeterministic Behavior in Static Analysis Tools". In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE). (Artifact) Github [https://github.com/UTD-FAST-Lab/NDSAStudy](https://github.com/UTD-FAST-Lab/NDSAStudy)
- Anderson Oliveira, João Correia, Wesley K. G. Assunção, Juliana Alves Pereira, Rafael de Mello, Daniel Coutinho, Caio Barbosa, Paulo Libório, and Alessandro Garcia. Understanding Developers’ Discussions and Perceptions on Non-functional Requirements: The Case of the Spring Ecosystem. In 2024 ACM Foundations of Software Engineering (FSE).(Artifact) Github [https://github.com/andersonjso/devs_discussions_perceptions](https://github.com/andersonjso/devs_discussions_perceptions)

#### Method Type: Qualitative

#####  Qualitative Survey

- Bianca Trinkenreich, Ricardo Britto, Marco A. Gerosa, and Igor Steinmacher. 2022. An empirical investigation on the challenges faced by women in the software industry: a case study. In 2022 ACM/IEEE 44th International Conference on Software Engineering: Software Engineering in Society (ICSE-SEIS). (Artifact) Figshare. [https://figshare.com/s/d1c3bd386083fa55104a](https://figshare.com/s/d1c3bd386083fa55104a)
- Hermann, K., Peldszus, S., Steghöfer, J.P. and Berger, T., 2025. An Exploratory Study on the Engineering of Security Features. In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE). (Artifact) Zenodo [https://doi.org/10.5281/zenodo.14237228](https://doi.org/10.5281/zenodo.14237228)

#### Method Type: Literature Review

#####  Systematic Review

- Teymourian, A., Webb, A. M., Gharaibeh, T., Ghildiyal, A., & Baggili, I. SoK: Come Together–Unifying Security, Information Theory, and Cognition for a Mixed Reality Deception Attack Ontology & Analysis Framework. In 2025 34th USENIX Security Symposium (USENIX Security). (Artifact) Zenodo [https://zenodo.org/records/14732980](https://zenodo.org/records/14732980)
- Schloegel, M., Bars, N., Schiller, N., Bernhard, L., Scharnowski, T., Crump, A., Ale-Ebrahim, A., Bissantz, N., Muench, M. and Holz, T. Sok: Prudent evaluation practices for fuzzing. In 2024 IEEE Symposium on Security and Privacy (SP). (Artifact) Github [https://github.com/fuzz-evaluator](https://github.com/fuzz-evaluator)

#### Method: Other

##### Replication

-  Baatartogtokh, Y., Cook, K. and Grubb, A.M. Exploring the Robustness of the Effect of EVO on Intention Valuation through Replication. In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE). (Artifact) Scholarworks [https://doi.org/10.35482/csc.001.2025](https://doi.org/10.35482/csc.001.2025)
