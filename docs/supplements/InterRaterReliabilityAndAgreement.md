# Inter-Rater Reliability and Agreement <sup><a class="footnote footnote_ref">1</a>)</sup>
&quot;The extent to which different raters assign the same precise value for each item being rated&quot; (Inter-Rater Agreement; IRA) and &quot;the extent to which raters can consistently distinguish between different items on a measurement scale&quot; (Inter-Rater Reliability; IRR)<sup><a class="footnote footnote_ref">2</a></sup>
## Application

Applies to studies in which one or more human raters (also called judges or coders) rate (or measure, label, judge, rank or categorize) one or more properties of one or more research objects.<sup><a class="footnote footnote_ref">3</a></sup>

## Are multiple raters needed?

There is no universal rule to determine when two or more raters are necessary. The following factors should be considered:

- **Controversiality:** the more potentially controversial the judgment, the more multiple raters are needed; e.g. recording the publication year of the primary studies in an SLR is less controversial than evaluating the elegance of a technical solution.
- **Practicality:**  the less practical it is to have multiple rates, the more reasonable a single-rater design becomes; e.g. multiple raters applying an _a priori_, deductive coding scheme to some artifacts is more practical than multiple raters inductively coding 2000 pages of interview transcripts.
- **Philosophy:** having multiple raters is more important from a realist ontological perspective (characteristic of positivism and falsificationism) than from an idealist ontological perspective (characteristic of interpretivism and constructivism).

## Specific Attributes

### Essential Attributes
- [ ] clearly states what properties were rated
- [ ] clearly states how many raters rated each property  

EITHER: 
  - [ ] provides reasonable justification for using a single rater<sup><a class="footnote footnote_ref">4</a></sup>

OR:  
  - [ ] describes the process by which two or more raters independently rated properties of research objects; AND  
  - [ ] describes how disagreements were resolved; AND  
  - [ ] indicates the variable type (nominal, ordinal, interval, ratio) of the ratings; AND  
  - [ ] reports an appropriate statistical measure of IRR/IRA<sup><a class="footnote footnote_ref">5</a></sup>  

### Desirable Attributes
- [ ] provides supplementary materials including: rating scheme(s), decision rules, example disagreements, IRR/IRA broken down by property or wave of analysis.
- [ ] justifies the statistic used<sup><a class="footnote footnote_ref">6</a></sup>
- [ ] reports established interpretations or thresholds for the statistics used
- [ ] analyzes anomalous results<sup><a class="footnote footnote_ref">7</a></sup> in light of the properties of the statistic used (e.g. Cohen&#39;s kappa anomalies<sup><a class="footnote footnote_ref">8</a></sup>)
- [ ] describes the raters&#39; training or experience in the research topic
- [ ] resolves disagreements through discussion (rather than voting)

### Extraordinary Attributes
- [ ] employs more than three raters per property
- [ ] reports an iterative process with multiple cycles of (i) rating a subset of the data, (ii) resolving disagreements, and (iii) updating the rating scheme or decision rules until a minimum threshold indicates acceptable reliability/agreement; reports IRR/IRA for each cycle in an iterative process
- [ ] calculates IRR/IRA for internal quality of the research, i.e., as a tool for progressively improving the consistency of rating systems thus improving researchers&#39; reflexivity

## Antipatterns

- Reporting IRR where IRA is more appropriate and vice versa.
- Pretending that the IRR or IRA statistic indicates good reliability when it is below established thresholds.
- Calculating multiple IRR/IRA measures and reporting only the most favourable (p-hacking)
- Pretending that an obviously positivist study adopts an idealist ontology to avoid employing multiple raters

## Examples of Acceptable Deviations

- Resolving disagreements using a tiebreaker instead of discussion because a rater was unavailable.
- Supplementary materials do not include example disagreements because the data is sensitive.
- There is no indication of the number of iterations performed to assess / improve the IRR / IRA.

## Invalid Criticisms

- Criticizing use of a single rater where multiple raters would be impractical or inconsistent with the study&#39;s underlying philosophy..
- Criticizing use of a single rater when the data is such that there is no reason to suspect different raters would reach different conclusions.
- Criticizing use of multiple raters. It is difficult to imagine a scenario in which multiple raters would actively harm a study&#39;s credibility.
- &#39;IRR/IRA is too low&#39; when there is no evidence-based threshold and reliability threats are clearly acknowledged.

## Notes

- IRR is a measure of correlation the can be calculated using (for example) Pearson&#39;s r, Kendall&#39;s tau or Spearman&#39;s rho
- IRA is a measure of agreement and can be calculated using (for example) Scott&#39;s π, Cohen&#39;s κ, Fleiss&#39;s κ and Krippendorff&#39;s α (for example).
- IRR/IRA analysis not only indicates reliability and objectivity (in positivist research) but also improves reflexivity (in anti-positivist research).

## Suggested Readings
FENG, G. C. 2014. Intercoder reliability indices: disuse, misuse, and abuse. _Quality &amp; Quantity_, 48 (3), 1803-1815.  

GISEV, N., BELL, J. S., &amp; CHEN, T. F. 2013. Interrater agreement and interrater reliability: key concepts, approaches, and applications. _Research in Social and Administrative Pharmacy_, 9 (3), 330-338.  

HENRICA C.W. DE VETA, CAROLINE B. TERWEEA, DIRK L. KNOLA,B, LEX M. BOUTER. 2006. When to use agreement versus reliability measures. _Journal of Clinical Epidemiology_ 59, 1033–1039.  

NILI, A., TATE, M., BARROS, A., &amp; JOHNSTONE, D. 2020. An approach for selecting and using a method of inter-coder reliability in information management research. _International Journal of Information Management_, 54.  

O&#39;CONNOR, C., &amp; JOFFE, H. 2020. Intercoder reliability in qualitative research: debates and practical guidelines. _International Journal of Qualitative Methods_, 19.  

## Exemplars
JESSICA DIAZ, DANIEL LÓPEZ-FERNÁNDEZ, JORGE PEREZ, ÁNGEL GONZÁLEZ-PRIETO (in press) Why are many businesses instilling a DevOps culture into their organization? _Empirical Software Engineering  

JORGE PÉREZ, JESSICA DÍAZ, JAVIER GARCÍA-MARTÍN, AND BERNARDO TABUENCA. 2020. Systematic literature reviews in software engineering - enhancement of the study selection process using Cohen&#39;s Kappa statistic. _Journal of Systems and Software_, 168  

JORGE PÉREZ, CARMEN VIZCARRO, JAVIER GARCÍA, AURELIO BERMÚDEZ, AND RUTH COBOS.2017. Development of Procedures to Assess Problem-Solving Competence in Computing Engineering. _IEEE Transactions on Education_, 60 (1), 22-28  

R. MOHANANI, B. TURHAN, P. RALPH, (in press) Requirements framing affects design creativity. _IEEE Transactions on Software Engineering._ DOI: 10.1109/TSE.2019.2909033  

ZAPF, A., CASTELL, S., MORAWIETZ, L., &amp; KARCH, A. 2016. Measuring inter-rater reliability for nominal data–which coefficients and confidence intervals are appropriate? _BMC medical research methodology_, 16, article 93  

---
<sup><a class="footnote footnote_text">1</a></sup> Assessing consistency among raters, where appropriate, promotes &quot;_systematicity, communicability and transparency of the coding process; reflexivity and dialogue within research teams; and helping to satisfy diverse audiences of the trustworthiness of the research_&quot; (O&#39;Connor &amp; Joffe 2020).<br>
<sup><a class="footnote footnote_text">2</a></sup> See Gisev, Bell, &amp; Chen (2013)<br>
<sup><a class="footnote footnote_text">3</a></sup> For example: (a) applying selection criteria in an SLR; (b) an experiment in which a dependent variable like code quality is scored by human; (c) deductive qualitative analysis with an a priori coding scheme in positivist case study.<br>
<sup><a class="footnote footnote_text">4</a></sup> e.g. the ratings are uncontroversial; multiple raters would be impractical; multiple raters are inconsistent with the philosophical perspective of the work<br>
<sup><a class="footnote footnote_text">5</a></sup> Such as Pearson&#39;s r, Kendall&#39;s tau, Spearman&#39;s rho, Cohen&#39;s Kappa, Fleiss Kappa, Krippendorff&#39;s Alpha or (rarely) percent agreement<br>
<sup><a class="footnote footnote_text">6</a></sup> E.g. is the absolute value for each item being rated (IRA) or the trend in ratings (IRR) important?<br>
<sup><a class="footnote footnote_text">7</a></sup> E.g. high value of the observed agreement, but low value of the statistic or vice versa<br>
<sup><a class="footnote footnote_text">8</a></sup> See FEINSTEIN, A. R., &amp; CICCHETTI, D. V.1990. High Agreement But Low Kappa: I. the Problems of Two Paradoxes\*. _J Clin Epidemiol_, 43(6), 543–549.
