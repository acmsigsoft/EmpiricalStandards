# Data Science
<standard name="Data Science">

<desc>Studies that analyze software engineering phenomena or artifacts using data-centric analysis methods such as machine learning or other computational intelligence appraches as well as search-based approaches<footnote><sup>[1](#myfootnote1)</sup></desc>

## Application

Applies to studies that primarily analyze existing software phenomena using predictive, preemptive or corrective modelling. 

- If the analysis focuses on the toolkit, rather that some new conclusions generated by the toolkit, consider the **Artifacts Standard** 
- If the analysis focuses on a single, context-rich setting (e.g., a detailed analysis of a single repository), consider the **Case Study Standard**.
- If the temporal dimension is analyzed, consider the **Longitudinal Studies Standard**. 
- If the data objects are discussions or messages between humans, consider the **Discourse Analysis Standard**.
- If data visualizations are used, consider the **Information Visualization Supplement**. (With large data sets especially, care is needed to keep visualizations legible.)
- If the analysis selects a subset of available data, consult the **Sampling Supplement**.

## Specific Attributes

### Essential Attributes
<checklist name="Essential">
	
<intro>

- [ ] explains why it is timely to investigate the proposed problem using the proposed method

<method>

- [ ] explains how and why the data was selected
- [ ] presents the experimental setup (e.g. using a dataflow diagram)<sup>[2](#myfootnote2)</sup>
- [ ] describes the feature engineering approaches<sup>[3](#myfootnote3)</sup> and transformations that were applied
- [ ] explains how the data was pre-processed, filtered, and categorized
- [ ] EITHER: discusses state-of-art baselines (and their strengths, weaknesses and limitations)
	OR: explains why no state-of-art baselines exist
	OR: provides compelling argument that direct comparisons are impractical
- [ ] defines the modeling approach(es) used (e.g. clustering then decision tree learning), typically using pseudocode
- [ ] discusses the hardware and software infrastructure used<sup>[4](#myfootnote4)</sup>
- [ ] justifies all statistics and (automated or manual) heuristics used 
- [ ] describes and justifies the evaluation metrics used	

<results>

- [ ] goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information

<discussion>

- [ ] discusses technical assumptions and threats to validity that are specific to data science<sup>[5](#myfootnote5)</sup>
	
<other>

</checklist>

### Desirable Attributes
<checklist name="Desirable">

- [ ] provides a replication package including source code and data set(s), or if data cannot be shared, synthetic data to illustrate the use of the algorithms<sup>[6](#myfootnote6)</sup>
- [ ] data is processed by multiple learners, of different types<sup>[7](#myfootnote7)</sup>
- [ ] data is processed multiple times with different, randomly selected, training/test examples; the results of which are compared via significance tests and effect size tests (e.g. cross-validation)
- [ ] carefully selects the hyperparameters that control the data miners (e.g. via analysis of settings in related work or some automatic hyperparameter optimizer such as grid search)
- [ ] manually inspects some non-trivial portion of the data (i.e. data sanity checks)
- [ ] clearly distinguishes evidence-based results from interpretations and speculation<sup>[8](#myfootnote8)</sup>
</checklist>

### Extraordinary Attributes
<checklist name="Extraordinary">

- [ ] leverages temporal data via longitudinal analyses (see the [Longitudinal Studies Standard](https://github.com/acmsigsoft/EmpiricalStandards/blob/master/docs/Longitudinal.md))
- [ ] triangulates with qualitative data analysis of selected samples of the data 
- [ ] triangulates with other data sources, such as surveys or interviews
- [ ] shares findings with and solicits feedback from the creators of the (software) artifacts being studied
</checklist>

## Examples of Acceptable Deviations

- Using lighter and less precise data processing (e.g. keyword matching or random subsampling) if the scale of data is too large for a precise analysis to be practical.
- Data not shared because it is impractical (e.g. too large) or unethical (e.g. too sensitive). Enough information should be offered to assure the reader that the data is real.
- Not using temporal analysis techniques such as time series when the data is not easily converted to time series (e.g. some aspects of source code evolution may not be easily modelled as time series).
- Not all studies need statistics and hypotheses. Some studies can be purely or principally descriptive.
- Different explanations have different requirements (e.g. hold out sets, cross-validation)<sup>[9](#myfootnote9)</sup>.

## Antipatterns

- Using statistical tests without checking their assumptions.
- Using Bayesian statistics without motivating priors. 
- Claiming causation without not only establishing covariaton and precedence but also eliminating third variable explanations and at least hypothesizing a generative mechanism.
- Pre-processing changes training and test data; e.g. while it may be useful to adjust training data class distributions via (say) sub-sampling of majority classes, that adjustment should not applied to the test data (since it is important to assess the learner on the kinds of data that might be seen “in the wild”).
- Unethical data collection or analysis (see the [Ethics (Secondary Data) supplement](https://github.com/acmsigsoft/EmpiricalStandards/blob/master/Supplements/EthicsSecondaryData.md))
- Significance tests without effect size tests; effect sizes without confidence intervals.
- Reporting a median, without any indication of variance (e.g., a boxplot).
- Conducting multiple trials without reporting variations between trials. 

## Invalid Criticisms 

-  You should have analyzed data ABC. The question reviewers should ask is whether the papers main claims are supported by the data that was analyzed, not whether some other data would have been better. 
- Does not have a reproduction package. These are desirable, not essential (yet).
- Findings are not actionable: not all studies may have directly actionable findings in the short term.
- "Needs more data" as a generic criticism without a clear, justified reason.
- Study does not use qualitative data.
- Study does not make causal claims, when it cannot.
- Study does not use the most precise data source, unless the data source is clearly problematic for the study at hand. Some data is impractical to collect at scale.

## Suggested Readings

1. Hemmati, Hadi, et al. "The msr cookbook: Mining a decade of research." 2013 10th Working Conference on Mining Software Repositories (MSR). IEEE, 2013.
2. Robles, Gregorio, and Jesus M. Gonzalez-Barahona. "Developer identification methods for integrated data from various sources." (2005).
3. Dey, Tapajit, et al. "Detecting and Characterizing Bots that Commit Code." arXiv preprint arXiv:2003.03172 (2020).
4. Hora, Andre, et al. "Assessing the threat of untracked changes in software evolution." Proceedings of the 40th International Conference on Software Engineering. 2018.
5. Herzig, Kim, and Andreas Zeller. "The impact of tangled code changes." 2013 10th Working Conference on Mining Software Repositories (MSR). IEEE, 2013.
6. Berti-Équille, L. (2007). Measuring and Modelling Data Quality for Quality-Awareness in Data Mining.. In F. Guillet & H. J. Hamilton (ed.), Quality Measures in Data Mining , Vol. 43 (pp. 101-126) . Springer . ISBN: 978-3-540-44911-9.
7. Wohlin, C., Runeson, P., Höst, M., Ohlsson, M. C.,, Regnell, B. (2012). Experimentation in Software Engineering.. Springer. ISBN: 978-3-642-29043-5Wohlin’ standard thrrs
8.  Raymond P. L. Buse and Thomas Zimmermann. 2012. Information needs for software development analytics. In Proceedings of the 34th International Conference on Software Engineering (ICSE '12). IEEE Press, 987–996.
9.  https://aaai.org/Conferences/AAAI-21/reproducibility-checklist/
10.  Baljinder Ghotra, Shane McIntosh, and Ahmed E. Hassan. 2015. Revisiting the impact of classification techniques on the performance of defect prediction models. In Proceedings of the 37th International Conference on Software Engineering - Volume 1 (ICSE '15). IEEE Press, 789–800.
11.  Daniel Russo and Klaas-Jan Stol. In press. PLS-SEM for Software Engineering Research: An Introduction and Survey. *ACM Computing Surveys*.  

## Exemplars

1. A. Barua, S. W. Thomas, A. E. Hassan, What are developers talkingabout? an analysis of topics and trends in stack overflow, Empirical Software Engineering 19 (3) (2014) 619–654.
2. Bird, C., Rigby, P. C., Barr, E. T., Hamilton, D. J., German, D. M., & Devanbu, P. (2009, May). The promises and perils of mining git. In 2009 6th IEEE International Working Conference on Mining Software Repositories (pp. 1-10). IEEE.
3. Kalliamvakou, E., Gousios, G., Blincoe, K., Singer, L., Germán, D. M. & Damian, D. E. (2014). The promises and perils of mining GitHub.. In P. T. Devanbu, S. Kim & M. Pinzger (eds.), MSR (p./pp. 92-101), : ACM. ISBN: 978-1-4503-2863-0
4. Herbsleb, J. & Mockus, A. (2003). An Empirical Study of Speed and Communication in Globally Distributed Software Development. IEEE Transactions on Software Engineering, 29, 481-94.2
4. Menzies, T., Greenwald, J., & Frank, A. (2006). Data mining static code attributes to learn defect predictors. IEEE transactions on software engineering, 33(1), 2-13.
5. Menzies, T., & Marcus, A. (2008, September). Automated severity assessment of software defect reports. In 2008 IEEE International Conference on Software Maintenance (pp. 346-355). IEEE.
6. Nair, V., Agrawal, A., Chen, J., Fu, W., Mathew, G., Menzies, T., Minku, L. L., Wagner, M. & Yu, Z. (2018). Data-driven search-based software engineering.. In A. Zaidman, Y. Kamei & E. Hill (eds.), MSR (p./pp. 341-352), : ACM.
7. Rahman, F., & Devanbu, P. (2013, May). How, and why, process metrics are better. In 2013 35th International Conference on Software Engineering (ICSE) (pp. 432-441). IEEE.
8. Tufano, M., Palomba, F., Bavota, G., Oliveto, R., Penta, M. D., Lucia, A. D. & Poshyvanyk, D. (2017). When and Why Your Code Starts to Smell Bad (and Whether the Smells Go Away).. IEEE Trans. Software Eng., 43, 1063-1088.

---
<footnote><sup>[1](#myfootnote1)</sup>Dhar, V. (2013). Data Science and Prediciton, Communications of the ACM, December 2013, Vol. 56 No. 12, Pages 64-73. https://cacm.acm.org/magazines/2013/12/169933-data-science-and-prediction/fulltext</footnote><br>
<footnote><sup>[2](#myfootnote2)</sup>Akidau, Tyler, Robert Bradshaw, Craig Chambers, Slava Chernyak, Rafael J. Fernández-Moctezuma, Reuven Lax, Sam McVeety et al. "The dataflow model: a practical approach to balancing correctness, latency, and cost in massive-scale, unbounded, out-of-order data processing." (2015). Proceedings of the VLDB Endowment 8.12</footnote><br>
<footnote><sup>[3](#myfootnote3)</sup>Acf. Nargesian, Fatemeh, Horst Samulowitz, Udayan Khurana, Elias B. Khalil, and Deepak S. Turaga. "Learning Feature Engineering for Classification." In Ijcai, pp. 2529-2535. 2017.</footnote><br>
<footnote><sup>[4](#myfootnote4)</sup>including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks</footnote><br>
<footnote><sup>[5](#myfootnote5)</sup> For example, failing to address variations in the size or complexity of training, testing and validation data sets. For more, see: Wohlin, C., Runeson, P., Höst, M., Ohlsson, M. C.,,Regnell, B. (2012). Experimentation in Software Engineering. Springer. ISBN: 978-3-642-29043-5</footnote><br>	
<footnote><sup>[6](#myfootnote6)</sup>Sarkar, T. (2019). Synthetic data generation - a must-have skill for new data scientists. (July 2019).https://towardsdatascience.com/synthetic-data-generation-a-must-have-skill-for-new-data-scientists-915896c0c1ae</footnote><br> 
<footnote><sup>[7](#myfootnote7)</sup>e.g. regression, bayes classifier, decision tree, random forests, SVM (maybe with different kernels); for guidance, see Baljinder Ghotra, Shane McIntosh, and Ahmed E. Hassan. 2015. Revisiting the impact of classification techniques on the performance of defect prediction models. In Proceedings of the 37th International Conference on Software Engineering - Volume 1 (ICSE '15). IEEE Press, 789–800.</footnote><br>
<footnote><sup>[8](#myfootnote8)</sup>Simply separating results and discussion into different sections is typically sufficient. No speculation in the results section.</footnote><br>
<footnote><sup>[9](#myfootnote9)</sup>c.f. Raymond P. L. Buse and Thomas Zimmermann. 2012. Information needs for software development analytics. In Proceedings of the 34th International Conference on Software Engineering (ICSE '12). IEEE Press, 987–996.</footnote><br>
</standard>
