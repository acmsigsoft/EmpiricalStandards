# Replication
<standard name="Replication">


*<desc>A study that deliberately repeats a previous study (the "original study") to determine whether its results can be reproduced.</desc>* (Carver et al., 2013)


## Application 

This standard applies to empirical studies that meet the following criteria:

- The replication attempt is deliberate and planned, not accidental overlap with a previous study.
- The original study is clearly identified as a separate previous publication. If the replication is not the only replication of the original study, i.e., it is a part of a *family of replications*, all the other replications are identified and the current replication is clearly defined in the context of the family.
 

## Definitions
  
To _reproduce_ means to repeat the original study's data analysis on the original study's data.
  
To _replicate_ means to repeat a study by collecting new data and repeating the original study's analysis on the new data.


## Specific Attributes
	
### Essential Attributes
	
<checklist name="Essential">

<intro>

- [ ] discusses the motivation for conducting the replication in appropriate detail (e.g., to validate the results, to broaden the results by changing the participant pool or the artifacts)
- [ ] defines the type of the replication by methodological similarity (exact, methodological, conceptual)<sup>[2](#footnote2)</sup>
- [ ] defines the type of the replication by overlap (partial, complete, extended)<sup>[3](#footnote3)</sup>
- [ ] defines the type of the replication by participants (internal, external, mixed)<sup>[4](#footnote4)</sup>

<method>

- describes the original study in appropriate detail, including:
  - [ ] the research questions of the original study
  - [ ] the design of the original study 
  - [ ] if applicable, the participants of the original study (their number and any relevant characteristics)
  - [ ] the artifacts of the original study<sup>[5](#footnote5)</sup>
  - [ ] the context variables of the original study that might have affected the design of the study or interpretation of the results
  - [ ] the major findings of the original study
- [ ] EITHER: describes overlap or interactions with the original study author(s)    
	OR: confirms that the original study author(s) were not involved in the replication
- [ ] describes and justifies any differences from the original study (design, participants, artifacts, procedures, data collection, or analysis)

<results>

- [ ] compares the results of the replication to the results of the original study
- [ ] clearly differentiates between results that are consistent and inconsistent with the original study

<discussion>

<conclusion>
  
<other>

</checklist>
     
### Desirable Attributes

<checklist name="Desirable">

- [ ] 	clearly seperates information about (i) the original study, (ii) the replication, (iii) the comparison of results, and (iv) conclusions <sup>[1](#footnote1)</sup>
- [ ] 	improves on the original study in some way (e.g., design, analysis, statistical power)
- for *families of replications*:
  	- [ ] provides a brief summary of the previous studies and replications, including information about how the studies were related, conclusions drawn and current state of knowledge about the topic 
	- [ ] places the results into the context of the entire family of studies 
  	- [ ] draws conclusions based on analysis of the results of the entire family of studies 	
- [ ] 	differentiates between minor and major differences when discussing inconsistent results, elaborates on whether the differences are reasonable and/or expected
- [ ]	 draws conclusions across the two (or more) studies; provides insights that would not have been evident from either study individually
- [ ] 	highlights any conclusions of the original study that were strengthened
- [ ] 	compares the limitations of the replication to the limitations of the original study
- [ ] 	proposes hypotheses about new context variables that may have become evident through the analysis of multiple studies

</checklist> 


<!-- ### Extraordinary Attributes
<checklist name="Extraordinary">


</checklist>  -->
	
## Antipatterns
	
- replicating an underpowered study with a second, equally-underpowered study
- repeating rather than correcting mistakes (e.g. using inappropriate statistical tests) in the original study 


## Invalid Criticisms

- The replication merely confirms the findings of the original study; no inconsistencies are reported.
- Failing to publish (e.g., in a replication package) materials owned by the original study authors who have not given permission to publish them. 
- Criticizing the research questions including their wording or formulation (since the research questions are set in the original study, the replication must follow them, even in conceptual replications).

## Suggested Readings

- Jeffrey C. Carver. 2010. [Towards reporting guidelines for experimental replications: A proposal](http://carver.cs.ua.edu/Papers/Conference/2010/2010_RESER.pdf). 1st International Workshop on Replication in Empirical Software Engineering Research. 2010. pp. 1–4.
- Jeffrey C. Carver, Natalia Juristo, Maria Teresa Baldassarre, and Sira Vegas. 2013. Replications of Software Engineering Experiments. Empirical Software Engineering 19, 2 (2013), 267–276. DOI:http://dx.doi.org/10.1007/s10664-013-9290-8 
- Alan Dennis and Joseph Valacich. 2014. A replication manifesto. AIS Transactions on Replication Research 1 (2014), 1–4. DOI:http://dx.doi.org/10.17705/1atrr.00001 
- Fabio Q. da Silva et al. 2012. Replication of empirical studies in Software Engineering Research: A systematic mapping study. Empirical Software Engineering (2012). DOI:http://dx.doi.org/10.1007/s10664-012-9227-7 
- Sarah Heckman, Jeffrey C. Carver, Mark Sherriff, and Ahmed Al-zubidy. 2021. A systematic literature review of empiricism and norms of reporting in Computing Education Research Literature. ACM Transactions on Computing Education 22, 1 (2021), 1–46. DOI:http://dx.doi.org/10.1145/3470652 
- Barbara Kitchenham. 2008. The role of Replications in empirical software engineering—a word of warning. Empirical Software Engineering 13, 2 (2008), 219–221. DOI:http://dx.doi.org/10.1007/s10664-008-9061-0 


## Exemplars

- Tomaž Kosar, Sašo Gaberc, Jeffrey C. Carver, and Marjan Mernik. 2018. Program comprehension of domain-specific and general-purpose languages: Replication of a family of experiments using Integrated Development Environments. Empirical Software Engineering 23, 5 (2018), 2734–2763. DOI:http://dx.doi.org/10.1007/s10664-017-9593-2 
- Faizan Khan, Istvan David, Daniel Varro, and Shane McIntosh. 2022. Code cloning in smart contracts on the Ethereum Platform: An extended replication study. IEEE Transactions on Software Engineering (2022), 1–13. DOI:http://dx.doi.org/10.1109/tse.2022.3207428 
- Davide Fucci and Burak Turhan. 2013. On the role of tests in test-driven development: A differentiated and partial replication. Empirical Software Engineering 19, 2 (2013), 277–302. DOI:http://dx.doi.org/10.1007/s10664-013-9259-7
- Cecilia Apa, Oscar Dieste, Edison G. Espinosa G., and Efraín R. Fonseca C. 2013. Effectiveness for detecting faults within and outside the scope of testing techniques: An independent replication. Empirical Software Engineering 19, 2 (2013), 378–417. DOI:http://dx.doi.org/10.1007/s10664-013-9267-7 
- Juha Itkonen and Mika V. Mäntylä. 2013. Are test cases needed? replicated comparison between exploratory and test-case-based software testing. Empirical Software Engineering 19, 2 (2013), 303–342. DOI:http://dx.doi.org/10.1007/s10664-013-9266-8 
- Marta N. Gómez and Silvia T. Acuña. 2013. A replicated quasi-experimental study on the influence of personality and team climate in software development. Empirical Software Engineering 19, 2 (2013), 343–377. DOI:http://dx.doi.org/10.1007/s10664-013-9265-9 

---
<footnote><sup>[1](#footnote1)</sup>See Carver et al. (2013).</footnote><br>
<footnote><sup>[2](#footnote2)</sup>Exact replication: same research questions, same method, same context. Methodological replication: same research questions, same method, different context. Conceptual replication: same research questions, different method, different context. (Dennis and Valacich, 2015)</footnote><br>
<footnote><sup>[3](#footnote3)</sup>Partial replication: addresses a subset of the original research questions. Complete replication: addresses each of the original reseach questions. Extended replication: addresses each of the original reseach questions and additional ones. (Carver, 2010)</footnote><br>
<footnote><sup>[4](#footnote4)</sup>Internal replication: the replicating team is the same as the original study's team. External replication: the replicating team is different from the original study's team. Mixed replication: overlaps exist between the replicating team and the original study's team. (da Silva et al., 2012)</footnote><br>
<footnote><sup>[5](#footnote5)</sup>Typical examples: input artifacts to experiments (such as questionnaires, user guides, etc.), output artifacts and data (interview transcripts, raw data, pre-processed data, etc.), analysis scripts, etc.</footnote><br>
